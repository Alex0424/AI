# Hallucinations

AI that gives wrong information

Reduce Hallucination with Prompt Engineering

Include
- Include instructions of requesting the model not to make up stuff but stay with facts.

Restrict
- Restrict the output

Add
- Add Chain of Thought style of instruction, “Solve the problem step by step.”

Repeat
- Repeat most important instructions in the prompt a couple of times.

Position
- Position most important instructions in the last making use of latency effect.

I dont know
- tell the prompt: say I don't know if you don't know the answer

Ground AI on the data you have lerned it
- if the data that i provide is not the company data say: i don't know